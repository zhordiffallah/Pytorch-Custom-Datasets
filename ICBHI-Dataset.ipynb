{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfp/QI1BYb7ORfGLcKXmls"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## ðŸš€ **Building a Custom DataLoader in PyTorch for the ICBHI Respiratory Sound Dataset**\n","\n","### **Overview**\n","This notebook guides you through the process of creating a custom DataLoader in PyTorch specifically for the ICBHI (International Conference on Biomedical Health Informatics) respiratory sound dataset. The goal is to efficiently load, process, and prepare this dataset for machine learning tasks, particularly for training and evaluating models that can diagnose respiratory conditions based on audio data.\n","\n","### **Objectives**\n","1. **Data Collection:**\n","   - Retrieve and organize respiratory sound audio files.\n","   - Extract patient identifiers from the file names to match them with other relevant data.\n","\n","2. **Data Integration:**\n","   - Load patient demographic data and diagnosis information.\n","   - Merge these data sources with the audio files to create a comprehensive dataset.\n","\n","3. **Custom DataLoader Creation:**\n","   - Implement a custom PyTorch `Dataset` class that handles the loading and preprocessing of the dataset.\n","   - Prepare the data for feeding into a deep learning model, ensuring that all necessary transformations are applied.\n","\n","### **Structure**\n","The notebook is structured as follows:\n","1. **Data Preparation:** Gathering and merging all relevant data sources.\n","2. **DataLoader Implementation:** Writing the custom PyTorch `Dataset` class.\n","3. **DataLoader Usage:** Demonstrating how to use the custom DataLoader in a PyTorch training loop.\n"],"metadata":{"id":"3qJ1v_KRKkK3"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import glob\n","import torch\n","import torchaudio\n","from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"Mnwv_Fg7Lc6e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQcp9fChY-V_"},"outputs":[],"source":["from google.colab import files\n","files.upload()  # Upload kaggle.json\n","# Import the 'files' module from Google Colab to handle file uploads.\n","# Call the 'upload' method from the 'files' module to prompt the user to upload the 'kaggle.json' file.\n","# This file contains the Kaggle API credentials necessary for accessing Kaggle datasets."]},{"cell_type":"code","source":["!mkdir -p ~/.kaggle\n","!mv /content/kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","!ls -la ~/.kaggle/\n","# Create a directory for Kaggle API configuration files in the user's home directory.\n","# Move the uploaded 'kaggle.json' file to the newly created '.kaggle' directory.\n","# Set permissions on 'kaggle.json' to read/write for the owner only (600), ensuring the file is secure.\n","# List the files in the '.kaggle' directory to confirm that 'kaggle.json' is correctly placed and has the proper permissions."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WsxsElx-cgdk","executionInfo":{"status":"ok","timestamp":1724929309620,"user_tz":-60,"elapsed":988,"user":{"displayName":"Zhor DIFFALLAH","userId":"15498836641066251630"}},"outputId":"fc368af1-9415-486f-8bc1-3547329addd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 16\n","drwxr-xr-x 2 root root 4096 Aug 29 11:01 .\n","drwx------ 1 root root 4096 Aug 29 11:01 ..\n","-rw------- 1 root root   69 Aug 29 11:01 kaggle.json\n"]}]},{"cell_type":"code","source":["# Use the Kaggle API command to download the 'Respiratory Sound Database' dataset.\n","# The dataset will be downloaded as a zip file to the current working directory.\n","!kaggle datasets download -d vbookshelf/respiratory-sound-database"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XD2zqeqMchDK","executionInfo":{"status":"ok","timestamp":1724929342943,"user_tz":-60,"elapsed":32912,"user":{"displayName":"Zhor DIFFALLAH","userId":"15498836641066251630"}},"outputId":"8501f025-8c46-40a5-8343-180b65c8c17b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/vbookshelf/respiratory-sound-database\n","License(s): unknown\n","Downloading respiratory-sound-database.zip to /content\n"," 99% 3.67G/3.69G [00:31<00:00, 158MB/s]\n","100% 3.69G/3.69G [00:31<00:00, 125MB/s]\n"]}]},{"cell_type":"markdown","source":["## ðŸ—‚ï¸ **Extracting the Respiratory Sound Database**\n","\n","Now that we've successfully downloaded the Respiratory Sound Database, the next step is to extract the contents of the zip file. ðŸ“¦\n","\n","The command below will:\n","\n","- **Unzip** the downloaded file, making the dataset's contents accessible in your Colab environment. ðŸ—ƒï¸\n","- **Extract** the files into the current working directory, so we can begin working with the data right away. ðŸ› ï¸"],"metadata":{"id":"NcYGakYaLGv9"}},{"cell_type":"code","source":["!unzip /content/respiratory-sound-database.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TrbbBM2FcmYS","executionInfo":{"status":"ok","timestamp":1724935656146,"user_tz":-60,"elapsed":1813,"user":{"displayName":"Zhor DIFFALLAH","userId":"15498836641066251630"}},"outputId":"b09056ac-78ff-40d8-a8b7-9bb725fa09a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/respiratory-sound-database.zip\n","replace Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/101_1b1_Al_sc_Meditron.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"markdown","source":["## ðŸ“Š **Data Loading and Preparation**\n","\n","In this section, we load and prepare the dataset by retrieving audio files and loading patient demographic and diagnosis data. The objective is to set up a comprehensive dataset that combines audio files with relevant patient information for further analysis or machine learning.\n","\n","1. **Headers Definition:**\n","   - First, we define a list of headers for the patient demographic data. This ensures that each column in the resulting DataFrame has a clear and meaningful name, which is essential for understanding and analyzing the data.\n","\n","2. **Retrieving Audio Files:**\n","   - We use the `glob` library to collect all `.wav` audio files from the specified directory. This method allows us to efficiently gather all audio data for the patients in one step. These audio files will later be linked to the patient data based on the patient number.\n","\n","3. **Loading Patient Demographic Data:**\n","   - Next, we load the patient demographic data from a `.txt` file using the `pandas` library. This data includes critical patient information such as age, sex, BMI, and, for children, weight and height. Since the file doesnâ€™t have headers, we manually add them to make the data easier to work with and more interpretable.\n","\n","4. **Loading Diagnosis Data:**\n","   - Finally, we load the diagnosis data from a CSV file. This data contains the diagnosis for each patient, which is crucial for our analysis and machine learning tasks. Like the demographic data, this file also lacks headers, so we add them manually to ensure clarity.\n","\n","Together, these steps prepare our dataset by combining the audio files with patient demographics and diagnosis information, creating a well-structured dataset ready for further processing or machine learning model training.\n"],"metadata":{"id":"_rLAxj77MN7s"}},{"cell_type":"code","source":["# Define headers for the patient demographic data\n","header = [\"Patient number\", \"Age\", \"Sex\", \"Adult BMI (kg/m2)\", \"Child Weight (kg)\", \"Child Height (cm)\"]\n","\n","# Step 1: Retrieve all audio files from the specified directory using glob\n","# This step gathers all .wav audio files (since the directory contains .txt and .wav files), which will be used in conjunction with the patient data\n","audio_files = glob.glob('/content/respiratory_sound_database/Respiratory_Sound_Database/audio_and_txt_files/*.wav')\n","\n","# Step 2: Load patient demographic data from a .txt file\n","# This data includes information such as age, sex, BMI, etc.\n","# We add column names using the predefined headers to make the data more interpretable\n","patient_data = pd.read_csv('/content/demographic_info.txt', delimiter='\\s+', header=None, names=header)\n","\n","# Step 3: Load diagnosis data from a CSV file\n","# The diagnosis data includes patient numbers and their corresponding diagnoses\n","# Adding headers ensures that each column is properly labeled for easier analysis\n","diagnosis_data = pd.read_csv('/content/respiratory_sound_database/Respiratory_Sound_Database/patient_diagnosis.csv', header=None, names=['Patient number', 'Diagnosis'])"],"metadata":{"id":"TrpwzfNF4k4n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ”„ **Merging Audio Files with Patient Data**\n","\n","In this section, we combine the audio file data with patient demographic and diagnosis information to create a comprehensive dataset. This is crucial for ensuring that each audio file is correctly linked with the relevant patient information, allowing for a more meaningful analysis and machine learning model training. What we aim to create is a sort of annotations file that links each audio file with the diagnosis corresponding to the patient number. This file will later be used as parameter to our custom dataset object.\n","\n","1. **Creating a DataFrame for Audio Files:**\n","   - We start by organizing the audio file data into a DataFrame, which will serve as the base for merging with patient demographic and diagnosis data.\n","\n","2. **Extracting the Filename from the Full Path:**\n","   - Since the audio file paths include directory information, we extract only the filename, which contains the patient number. This step simplifies the process of linking the audio files with patient data.\n","\n","3. **Extracting the Patient Number:**\n","   - The patient number, crucial for merging data, is embedded within the audio file names. We extract this number to use as a key for merging the audio data with the patient demographic and diagnosis information.\n","\n","4. **Merging with Diagnosis Data:**\n","   - We merge the audio file DataFrame with the diagnosis data using the patient number as the key. This ensures that each audio file is linked with the correct diagnosis, providing a foundation for building diagnostic models.\n","\n","5. **Optionally Merging with Patient Demographic Data:**\n","   - If additional patient details are needed, we also merge the audio file data with the demographic data. This enriches the dataset with more context, such as age, sex, and BMI, which can be valuable for further analysis.\n","\n","6. **Final Dataset Preparation:**\n","   - After merging, we display the final DataFrame to ensure that the data is correctly combined. Finally, we save the merged dataset to a CSV file, which can be used for further processing or as input for machine learning models.\n"],"metadata":{"id":"aHZpkbtWNTyk"}},{"cell_type":"code","source":["# Step 1: Create a DataFrame containing the audio file names\n","audio_df = pd.DataFrame(audio_files, columns=['AudioFile'])\n","\n","# Step 2: Extract the filename from the full path\n","# We use the basename function to isolate the filename, removing the directory path\n","audio_df['AudioFile'] = audio_df['AudioFile'].apply(lambda x: os.path.basename(x))\n","\n","# Step 3: Extract the patient number from the audio file name\n","# The patient number is embedded in the filename; we extract the first three digits and convert them to integers\n","audio_df['Patient number'] = audio_df['AudioFile'].str.extract(r'(\\d{3})').astype(int)\n","\n","# Step 4: Merge the audio DataFrame with the diagnosis data\n","# This step links each audio file with the corresponding patient diagnosis based on the patient number\n","merged_df = pd.merge(audio_df, diagnosis_data, on='Patient number', how='left')\n","\n","# Step 5: Optionally merge the resulting DataFrame with the patient demographic data\n","# This provides additional context such as age, sex, and BMI for each patient\n","merged_df = pd.merge(merged_df, patient_data, on='Patient number', how='left')\n","\n","# Step 6: Display the merged DataFrame to verify the result\n","print(merged_df)\n","\n","# Step 7: Save the merged DataFrame to a CSV file for further use\n","merged_df.to_csv('merged_data.csv', index=False)\n"],"metadata":{"id":"fjDmyOdALZz-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ›ƒ **CustomDataset Class**\n","\n","The `CustomDataset` class is designed to facilitate working with audio data in PyTorch. It handles loading audio files and their corresponding labels, making it easier to integrate custom datasets into PyTorch's data loading pipeline.\n","\n","- **Initialization (`__init__` method):**\n","  The class is initialized with:\n","  - The path to the directory containing audio files.\n","  - A CSV file containing labels for each audio file.\n","  - An optional transformation function to be applied to the audio data.\n","\n","- **Dataset Length (`__len__` method):**\n","  This method returns the total number of samples in the dataset, which corresponds to the number of rows in the labels CSV file.\n","\n","- **Fetching Items (`__getitem__` method):**\n","  Given an index, this method:\n","  - Retrieves the file path of the audio sample.\n","  - Loads the audio file using `torchaudio`.\n","  - Extracts the corresponding label from the labels DataFrame.\n","  - Returns a tuple containing the audio signal and its label.\n","\n","- **Audio Path Retrieval (`get_audio_path` method):**\n","  Constructs and returns the full file path to an audio sample based on its index and the base directory where audio files are stored.\n","\n","- **Label Retrieval (`get_audio_label` method):**\n","  Extracts and returns the label for a given audio sample based on its index in the labels DataFrame.\n","\n","This class provides a structured way to manage and preprocess audio data for machine learning tasks using PyTorch.\n"],"metadata":{"id":"rKc2inr0O9FE"}},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, data, labels, transform=None):\n","        \"\"\"\n","        Initializes the CustomDataset.\n","\n","        Parameters:\n","        - data (str): Directory path where audio files are stored.\n","        - labels (str): Path to the CSV file containing labels for the audio files.\n","        - transform (callable, optional): Optional transformation function to apply to the audio data.\n","        \"\"\"\n","        self.data = data\n","        self.labels = pd.read_csv(labels)  # Load labels from the CSV file into a DataFrame\n","        self.transform = transform\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the total number of samples in the dataset.\n","\n","        Returns:\n","        - int: Number of samples, which is the length of the labels DataFrame.\n","        \"\"\"\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieves an audio sample and its label based on the index.\n","\n","        Parameters:\n","        - idx (int): Index of the sample to retrieve.\n","\n","        Returns:\n","        - tuple: (signal, label) where `signal` is the loaded audio signal and `label` is the corresponding label.\n","        \"\"\"\n","        audio_path = self.get_audio_path(idx)  # Get the file path for the audio sample\n","        label = self.get_audio_label(idx)      # Get the label for the audio sample\n","        signal, sr = torchaudio.load(audio_path)  # Load the audio file\n","        return signal, label\n","\n","    def get_audio_path(self, idx):\n","        \"\"\"\n","        Constructs the file path for an audio sample based on its index.\n","\n","        Parameters:\n","        - idx (int): Index of the sample.\n","\n","        Returns:\n","        - str: Full path to the audio file.\n","        \"\"\"\n","        path = os.path.join(self.data, self.labels.iloc[idx, 0])  # Join directory path with filename from labels DataFrame\n","        return path\n","\n","    def get_audio_label(self, idx):\n","        \"\"\"\n","        Retrieves the label for a given audio sample based on its index.\n","\n","        Parameters:\n","        - idx (int): Index of the sample.\n","\n","        Returns:\n","        - str: Label for the audio sample.\n","        \"\"\"\n","        label = self.labels.iloc[idx, 2]  # Extract label from the labels DataFrame\n","        return label\n"],"metadata":{"id":"iMvUBFJrmo0N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ§° **Dataset usage**\n","\n","In this section, we perform the following steps:\n","\n","1. **Define Paths:**\n","   - Specify the directory where the audio files are stored.\n","   - Provide the path to the CSV file containing the annotations (labels) for these audio files.\n","\n","2. **Initialize the Dataset:**\n","   - Create an instance of the `CustomDataset` class using the defined paths for audio data and annotations.\n","\n","3. **Print Dataset Size:**\n","   - Output the total number of samples in the dataset. This helps verify that the dataset is loaded correctly and provides insight into its size.\n","\n","This process ensures that the dataset is properly set up and gives a clear understanding of the amount of data available for analysis or model training.\n"],"metadata":{"id":"NHkP21oWP89A"}},{"cell_type":"code","source":["# Define the path to the directory containing audio files\n","audio_data_path = '/content/respiratory_sound_database/Respiratory_Sound_Database/audio_and_txt_files'\n","\n","# Define the path to the CSV file containing annotations (labels) for the audio files\n","annotations_path = '/content/merged_data.csv'\n","\n","# Create an instance of the CustomDataset class with the specified paths\n","icbhidataset = CustomDataset(audio_data_path, annotations_path)\n","\n","# Print the total number of samples in the dataset\n","print(f\"There are {len(icbhidataset)} samples in the dataset\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Qi3j5e19UUP","executionInfo":{"status":"ok","timestamp":1724937156819,"user_tz":-60,"elapsed":341,"user":{"displayName":"Zhor DIFFALLAH","userId":"15498836641066251630"}},"outputId":"10ef4221-e3fe-4ce9-8598-922d1b057900"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 920 samples in the dataset\n"]}]},{"cell_type":"markdown","source":["## âŒ¨ **Accessing and Inspecting a Dataset Sample**\n","\n","In this section, we perform the following steps to inspect a sample from the dataset:\n","\n","1. **Retrieve a Sample:**\n","   - We access the first sample from the dataset using `ICBHI[0]`. This retrieves a tuple consisting of the audio signal and its corresponding label.\n","\n","2. **Inspect Signal and Label:**\n","   - We then print the shape of the audio signal and the label associated with this sample. This helps us understand the structure of the audio data (e.g., its dimensions) and verify the label information.\n","\n","By examining a sample, we gain insights into the format of the audio data and the type of labels provided, which is crucial for ensuring data consistency and preparing for further analysis or model training.\n"],"metadata":{"id":"unDcn67lQSYc"}},{"cell_type":"code","source":["# Retrieve the first sample from the dataset\n","signal, label = icbhidataset[56]\n","\n","# Print the shape of the audio signal and the corresponding label\n","signal.shape, label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQlgthtl_FzM","executionInfo":{"status":"ok","timestamp":1724937159378,"user_tz":-60,"elapsed":350,"user":{"displayName":"Zhor DIFFALLAH","userId":"15498836641066251630"}},"outputId":"e8ab45c3-782a-4b3b-c49c-991abc781a1b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 882000]), 'COPD')"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","source":["## ðŸ‘ **Conclusion**\n","By completing these steps, weâ€™ve established a solid foundation for working with the respiratory sound dataset. This preparation is crucial for subsequent stages such as data preprocessing, feature extraction, and model training. With the dataset correctly set up and verified, we can now proceed to further analysis and experimentation to advance our research or project objectives. Feel free to build on this setup with additional analysis, feature extraction, or model training as needed."],"metadata":{"id":"Had2_KxBQ0Ua"}}]}